---
title: "ZFS Overview"
description: "This document provides a comprehensive overview of ZFS, detailing its reliability, incredible capabilities, and essential features like data synchronization through snapshots. It also covers learning resources, useful commands, fine-tuning tips, and integration with Proxmox and Docker."
---

import { Callout } from "fumadocs-ui/components/callout";
import { Card, Cards } from "fumadocs-ui/components/card";
import { Steps, Step } from "fumadocs-ui/components/steps";
import { Tabs, Tab } from "fumadocs-ui/components/tabs";

# ZFS Overview

## Why ZFS?

<Callout type="info">
  We use a lot ZFS for our data for its reliability and incredible capabilities.
  The most important feature is data synchronization through snapshots. Clone
  also enables to easily have same data as production for tests.
</Callout>

## Learning Resources

<Cards>
  <Card title="ZFS Introduction" href="/docs/Infra/reports/2023/2023-02-24-zfs-introduction">
    The onboarding made by Christian - great starting point
  </Card>
  <Card
    title="OpenZFS Official Documentation"
    href="https://openzfs.github.io/openzfs-docs/"
  >
    Very complete documentation (though a bit hard to navigate)
  </Card>
  <Card
    title="Proxmox ZFS Documentation"
    href="https://pve.proxmox.com/pve-docs/pve-admin-guide.html#chapter_zfs"
  >
    Proxmox-specific ZFS information and Wiki
  </Card>
  <Card
    title="Ubuntu ZFS Tutorial"
    href="https://ubuntu.com/tutorials/using-zfs-snapshots-clones#1-overview"
  >
    Tutorial about ZFS snapshots and clone
  </Card>
  <Card
    title="ZFS Cheat Sheet"
    href="https://blog.mikesulsenti.com/zfs-cheat-sheet-and-guide/"
  >
    A good cheat-sheet for quick reference
  </Card>
</Cards>

### Key Documentation Sections

<Cards>
  <Card 
    title="Misc Man Pages" 
    href="https://openzfs.github.io/openzfs-docs/man/master/7/index.html"
    description="Contains a lot of useful information on properties and features"
  />
  <Card 
    title="Basic Concepts" 
    href="https://openzfs.github.io/openzfs-docs/Basic%20Concepts/index.html"
    description="A good place to return to from time to time to grab concepts better"
  />
  <Card 
    title="System Administration" 
    href="https://openzfs.github.io/openzfs-docs/man/master/8/index.html"
    description="Man pages on system administration commands are really useful"
  />
  <Card 
    title="Performance and Tuning" 
    href="https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/index.html"
    description="Valuable to dig in some options for optimization"
  />
</Cards>

## Useful Commands

<Steps>
<Step>
**Pool Status**
```bash
zpool status
```
See eventual errors in your ZFS pools.
</Step>

<Step>
**Device List**
```bash
zpool list -v
```
See all devices in your pools.
</Step>

<Step>
**Dataset List**
```bash
zfs list -r
```
Get all datasets and their mountpoints.
</Step>

<Step>
**I/O Statistics**
```bash
zpool iostat
```
See stats about read/write operations.
</Step>

<Step>
**Pool History**
```bash
zpool history
```
List all operations done on a pool.
</Step>

<Step>
**Space Usage**
```bash
zpool list -o name,size,usedbysnapshots,allocated
```
See space allocated in your pools.
</Step>
</Steps>

### Important Command Notes

<Callout type="warning">
  **ALLOC Quirk**: There is a quirk with ALLOC which is different for mirror
  pools and raidz pools. On the first it's allocated space to datasets, on the
  second it's used space.
</Callout>

**Free Space**: `df` on a dataset does not really work because free space is shared between the datasets. You can still see datasets usage by using:

```bash title="Dataset Usage"
zfs list -o name,used,usedbydataset,usedbysnapshots,available -r <pool_name>
```

### Advanced Commands

<Tabs items={["iostat", "zdb"]}>
<Tab value="iostat">
```bash title="Detailed I/O Stats"
zpool iostat -vl 5  # Particularly useful
zpool iostat -w     # Helps understand time taken by data to be read/written
```
</Tab>

<Tab value="zdb">
```bash title="ZFS Database Info"
zdb -s <poolname>  # Pool statistics
```

[`zdb`](https://openzfs.github.io/openzfs-docs/man/master/8/zdb.8.html) is also worth knowing for debugging

</Tab>
</Tabs>

## Fine-Tuning

<Cards>
  <Card title="Compression">
    We use compression on a lot of our datasets (`compression=on`)
  </Card>
  <Card title="Cache Optimization">
    You can set `primarycache=metadata` for volumes with large number of files
    that are not critical to read fast (e.g., Images on off container, or
    backups from other servers)
  </Card>
  <Card title="Performance Monitoring">
    In [Munin](/docs/Infra/monitoring/munin) `ZFS ARC - Hit ratio` the `ZFS Arc
    Efficiency` gives you an idea of how well ZFS caching is performing
  </Card>
</Cards>

## Integration with Other Systems

<Tabs items={["proxmox", "sanoid"]}>
<Tab value="proxmox">
<Callout type="info">
Proxmox uses ZFS to replicate containers and VMs between servers. It also uses it to backup data.

**Note**: We progressively abandon Proxmox ZFS replication in favor of using sanoid/syncoid.

</Callout>
</Tab>

<Tab value="sanoid">
<Card title="Sanoid Integration" href="/docs/Infra/storage/sanoid">
We use sanoid / syncoid to sync ZFS datasets between servers (also to back them up)
</Card>
</Tab>
</Tabs>

## How-To Guides

### How to Create a ZPool

<Steps>
<Step>
**Check Disk Availability**
Ensure disks are not mounted and available (e.g., using `lsblk`). You can eventually split them into different partitions if needed.
</Step>

<Step>
**Create the Pool**
```bash title="Create ZPool"
zpool create <pool_name> <pool-type> <device1> <device2> ...
```

- For pool name use something like zfs-something (zfs-hdd, zfs-nvme, etc.)
- For pool-type, use the one that's needed: none (no mention), mirror, raidz

</Step>

<Step>
**Optional: Add Cache/Log Devices**
You may add a [cache](https://openzfs.github.io/openzfs-docs/man/master/7/zpoolconcepts.7.html#Cache_Devices) and/or a [log device](https://openzfs.github.io/openzfs-docs/man/master/7/zpoolconcepts.7.html#log).
</Step>
</Steps>

### How to NFS Mount a ZFS Dataset

<Callout type="info">ZFS directly integrates to NFS server.</Callout>

<Steps>
<Step>
**Set ShareNFS Property**
To have a dataset shared in NFS, you have to set sharenfs property to allowed addresses and other options.

Examples:

```bash title="NFS Sharing Examples"
zfs set sharenfs="rw=@10.0.0.0/28,no_root_squash" <pool_name>/<dataset_name>
zfs set sharenfs="rw=@10.0.0.1/32,rw=@10.1.0.200/32,no_root_squash" <pool_name>/<dataset_name>
```

</Step>
</Steps>

<Callout type="error">
**Very important**: Always filter on an internal sub network, otherwise your Dataset is exposed to the internet!!!

Beware that all descendants inherit the property and will also be shared.

</Callout>

### How to Replace a Disk in ZPool

<Steps>
<Step>
**List Devices**
Get a list of devices using `zpool status <pool_name>`
</Step>

<Step>
**Put Disk Offline**
```bash title="Offline Disk"
zpool <pool_name> offline <device_name>  # e.g., zpool rpool offline sdf
```
</Step>

<Step>**Replace Physical Disk** Replace the disk physically</Step>

<Step>
**Replace in ZPool**
```bash title="Replace in ZPool"
zpool <pool_name> replace /dev/<device_name>  # e.g., zpool rpool replace /dev/sdf
```
</Step>

<Step>
**Verify Resilvering**
Verify disk is back and being resilvered:

```bash title="Check Status"
zpool status <pool_name>
```

Expected output showing resilvering:

```
 state: DEGRADED
status: One or more devices is currently being resilvered...
  scan: resilver in progress since ...
...
        replacing-5  DEGRADED     0     0     0
        old        OFFLINE      0     0     0
        sdf        ONLINE       0     0     0  (resilvering)
```

</Step>

<Step>
**Optional: Run Scrub**
After resilver finishes, you can eventually run a scrub:

```bash title="Run Scrub"
zpool scrub <pool_name>
```

</Step>
</Steps>

### How to Sync ZFS Datasets

<Callout type="info">
  To Sync ZFS you just take snapshots on the source at specific intervals (we
  use cron jobs). You then use
  [zfs-send](https://openzfs.github.io/openzfs-docs/man/8/zfs-send.8.html) and
  [zfs-recv](https://openzfs.github.io/openzfs-docs/man/8/zfs-recv.8.html)
  through ssh to sync the distant server (send snapshots).
</Callout>

<Tabs items={["automatic", "manual"]}>
<Tab value="automatic">
<Card title="Automated Syncing" href="/docs/Infra/storage/sanoid">
We normally do it using sanoid and syncoid
</Card>

Proxmox might also do it as part of corosync to replicate containers across cluster.

</Tab>

<Tab value="manual">
```bash title="Manual ZFS Sync"
zfs send <previous-snap> <dataset_name>@<last-snap> \
  | ssh <hostname> zfs recv <target_dataset_name> -F
```

<Card
  title="STO Products Sync"
  href="https://github.com/openfoodfacts/openfoodfacts-infrastructure/blob/develop/scripts/off1/sto-products-sync.sh"
>
  ZFS sync of sto files from off1 to off2
</Card>

<Callout type="warning">
You also have to clean snapshots from time to time to avoid retaining too much useless data.

On ovh3: [snapshot-purge.sh](https://github.com/openfoodfacts/openfoodfacts-infrastructure/blob/develop/scripts/ovh3/snapshot-purge.sh)

</Callout>
</Tab>
</Tabs>

### How to Docker Mount a ZFS Dataset

<Cards>
  <Card title="Local Mount">
    If ZFS dataset is on same machine we can use bind mounts to mount a folder
    in a ZFS partition
  </Card>
  <Card title="Remote Mount">
    For distant machines, ZFS datasets can be exposed as NFS partition. Docker
    has an integrated driver to mount distant NFS as volumes
  </Card>
</Cards>

### How to Mount Datasets in a Proxmox Container

To mount dataset in a proxmox container you have two options:

<Tabs items={["shared-disk", "bind-volumes"]}>
<Tab value="shared-disk">
<Callout type="info">
Not really experimented yet, but it could have the advantage to enable replication.
</Callout>

<Steps>
<Step>
On your VM/CT, in resource, add a disk.
</Step>
<Step>
Add the mountpoint to your disk and declare it shared.
</Step>
<Step>
In another VM/CT, add the same disk.
</Step>
</Steps>
</Tab>

<Tab value="bind-volumes">
See: https://pve.proxmox.com/wiki/Linux_Container#_bind_mount_points and https://pve.proxmox.com/wiki/Unprivileged_LXC_containers

<Steps>
<Step>
**Edit Container Configuration**
Edit `/etc/pve/lxc/<container_id>.conf` and add volumes with mount point.

Example:

```bash title="Container Configuration"
# volumes
mp0: /zfs-hdd/opff,mp=/mnt/opff
mp1: /zfs-hdd/opff/products/,mp=/mnt/opff/products
mp2: /zfs-hdd/off/users/,mp=/mnt/opff/users
mp3: /zfs-hdd/opff/images,mp=/mnt/opff/images
mp4: /zfs-hdd/opff/html_data,mp=/mnt/opff/html_data
mp5: /zfs-hdd/opff/cache,mp=/mnt/opff/cache
```

<Callout type="warning">
**Important**: If you have nested mount points, the order is very important. First the outermost, then the inner ones.
</Callout>
</Step>

<Step>
**Reboot Container**
To take changes in account, you have to reboot:

```bash title="Reboot Container"
pct reboot <container_id>
```

</Step>
</Steps>
</Tab>
</Tabs>

#### Getting UIDs and GIDs Right

<Callout type="info">
  LXC maps uids inside the container to specific ids outside, most of the time
  by adding a large value. It's a way to ensure security.
</Callout>

<Steps>
<Step>
**Edit Sub UID/GID**
If you want to have file belonging to say uid 1000 in the zfs mount, you will have to tweak it:

We edit `/etc/subuid` and `/etc/subgid` to add `root:1000:10`. This allows container started by root to map ids 1000 to their same ids on system.

</Step>

<Step>
**Configure Container ID Mapping**
Edit `/etc/pve/lxc/<machine_id>.conf` to add sub_id exceptions:

```bash title="UID/GID Mapping"
# uid map: from uid 0 map 999 uids (in the ct) to the range starting 100000 (on the host)
# so 0..999 (ct) → 100000..100999 (host)
lxc.idmap = u 0 100000 999
lxc.idmap = g 0 100000 999
# we map 10 uid starting from uid 1000 onto 1000, so 1000..1010 → 1000..1010
lxc.idmap = u 1000 1000 10
lxc.idmap = g 1000 1000 10
# we map the rest of 65535 from 1010 upto 101010, so 1010..65535 → 101010..165535
lxc.idmap = u 1011 101011 64525
lxc.idmap = g 1011 101011 64525
```

</Step>
</Steps>
