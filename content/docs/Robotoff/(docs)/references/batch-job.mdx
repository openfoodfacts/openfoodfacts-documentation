---
title: "Google Batch Job"
description: "This document details Robotoff's Google Batch Job feature, which processes large product volumes using cloud resources. It covers the architecture, including job launching, configuration files, container registry, batch job stages, storage, and importing processed data, along with roles and additional notes."
---

import { Callout } from "fumadocs-ui/components/callout";
import { Card, Cards } from "fumadocs-ui/components/card";
import { Steps, Step } from "fumadocs-ui/components/steps";
import AnimatedSvg from "@/components/AnimatedSvg";

# Google Batch Job

Robotoff primarily provides models to Open Food Facts with real-time inference using Nvidia Triton Inference on CPUs.

However, this approach presents two major gaps:

<Cards>
  <Card
    title="Volume Processing"
    description="Challenges in processing large volumes of products during extended job runs"
  />
  <Card
    title="Resource Limitations"
    description="Limited access to larger computing resources, such as GPUs or multi-CPU setups"
  />
</Cards>

To fill these gaps, we integrated a batch job feature into Robotoff, leveraging the capabilities of Google Cloud Platform.

## Architecture

<AnimatedSvg
  src="../assets/batch_job_robotoff.svg"
  alt="Robotoff Architecture"
/>

The batch job pipeline is structured as follows:

<Steps>
  <Step>
    ### Launch Job

    The role of this command is to prepare and launch the job in the cloud. The launch depends on the type of job to perform, such as `ingredients-spellcheck`. Therefore, it takes as parameter `job_type`.

    Depending on the job type, the command will be responsible for:

    - Generate the google credentials from the production environment variables
    - Extracting, preparing and storing the data to process
    - Query the config file relative to the job and validate it using [Pydantic](https://docs.pydantic.dev/latest/)
    - Launch the google batch job

    <Callout type="info">
      The command can be found as a Command `launch_batch_job` in the CLI directory (see `./robotoff/cli/main.py`)
    </Callout>

  </Step>

  <Step>
    ### Config Files

    The configuration file defines the resources and setup allocated to the google batch job. Each batch job requires a unique configuration, stored as a YAML file.

    **Configuration includes:**
    - The resources location
    - The type and number of resources allocated
    - The maximal run duration
    - The number of tasks in parallel
    - The number of retries

    <Callout type="info">
      Configuration files are stored in `./robotoff/batch/configs/job_configs`
    </Callout>

    When initiating a job, the configuration is validated using the [Pydantic](https://docs.pydantic.dev/latest/) library. This process serves two purposes:

    <Cards>
      <Card title="Error Prevention" description="Prevents errors that could potentially cause the pipeline to fail" />
      <Card title="Cost Protection" description="Safeguards against the allocation of unnecessarily expensive resources" />
    </Cards>

    <Callout type="info">
      For more information about Google Batch Job configuration, check the [official documentation](https://cloud.google.com/batch/docs/reference/rest/v1/projects.locations.jobs).
    </Callout>

  </Step>

  <Step>
    ### Container Registry

    The container registry represents the core of the batch job. It contains the required dependencies and algorithms.

    Docker images are maintained independently of the Robotoff micro-service. Each directory contains the files with their `Dockerfile`. Once built, the Docker image is pushed manually using the Make command written in the Makefile, such as `deploy-spellcheck`.

    <Callout type="info">
      Docker files are located in the `./batch/` directory
    </Callout>

    **Available Registries:**
    - Google Artifact Registry within the project `Robotoff`
    - Docker hub
    - Public GitHub repository, such as [Robotoff](https://github.com/openfoodfacts/robotoff/tree/main)

    <Callout type="warn">
      The container needs to be accessible from the batch job once launched.
    </Callout>

  </Step>

  <Step>
    ### Batch Job Execution

    Once launched, the batch job goes through different stages:

    <Cards>
      <Card title="SCHEDULED" description="Job is scheduled for execution" />
      <Card title="QUEUED" description="Job is waiting in the queue" />
      <Card title="RUNNING" description="Job is currently executing" />
      <Card title="SUCCEEDED" description="Job completed successfully" />
      <Card title="FAILED" description="Job encountered an error" />
    </Cards>

    Each batch job is identified as the job type name associated with the launch `datetime`.

    <Callout type="info">
      During the run, all logs are stored in the Batch Job logs file.

      The list of batch jobs are located in the [Robotoff Google Batch Job](https://console.cloud.google.com/batch/jobs?referrer=search&project=robotoff).
    </Callout>

  </Step>

  <Step>
    ### Storage

    If the batch job requires to import or export data, we use a storage feature such as Google Storage as an interface between Robotoff and the job running in the cloud.

    <Callout type="info">
      **Google Storage Setup:**
      - **Robotoff side**: Google credentials are necessary
      - **Batch Job side**: Uses the default [service account](https://cloud.google.com/iam/docs/service-account-overview) associated with the project `Robotoff`, no additional setup required
    </Callout>

  </Step>

  <Step>
    ### Import Processed Data

    Once the job is successfully finished, the Robotoff API endpoint is queried from the job run through with an HTTP request.

    The role of this endpoint is to load the data processed by the batch job and import the new *predictions* to the Robotoff database.

    <Callout type="info">
      Check this [page](/docs/Robotoff/explanations/predictions) to understand the process Robotoff uses to transform raw *Predictions* into *Insights*.
    </Callout>

    **Security:**
    Since this endpoint has only the vocation of importing the batch job results, it is secured with a `BATCH_JOB_KEY` from external requests. This secured endpoint follows a [Bearer Authentication](https://swagger.io/docs/specification/authentication/bearer-authentication/). The key is set as an environment variable in Robotoff and is defined as batch environment variable during a job launch.

    <Callout type="info">
      Each batch job has its own method to import, or not, the results of the batch job.
    </Callout>

  </Step>
</Steps>

## Required Roles

To launch a batch job and import its results, the following roles need to be set up:

<Cards>
  <Card
    title="Artifact Registry Editor"
    description="Push Docker Image to the project image registry"
  />
  <Card title="Batch Job Editor" description="Manage batch job operations" />
  <Card
    title="Service Account User"
    description="Use service accounts for job execution"
  />
  <Card title="Storage Admin" description="Manage storage resources" />
</Cards>

<Callout type="warn">
  For production, it is preferable to create a custom *Service account* with
  these roles.
</Callout>

## Additional Notes

### Useful Links

Check the official Google Batch Job documentation:

<Cards>
  <Card
    title="Batch Job"
    href="https://cloud.google.com/batch/docs/get-started"
    description="Get started with Google Batch Job"
  />
  <Card
    title="Python API"
    href="https://cloud.google.com/python/docs/reference/batch/latest"
    description="Google Batch Job Python API reference"
  />
  <Card
    title="Python Examples"
    href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/batch"
    description="Batch job with Python examples"
  />
</Cards>

### Performance & Cost Notes

<Callout type="info">
  **Regional Considerations:** - Netherlands (europe-west4) has GPUs (A100, L4)
  - Add custom storage capacity to host heavy docker images (~24GB) by adding
  `BootDisk`
</Callout>

**Performance Benchmarks:**

- 1000 products processed: 1:30min (g2-instance-with 8) (overall batch job: 3:25min)
- L4: g2-instance-8 hourly cost: $0.896306 â‰ˆ ~$0.05 to process batch of 1000
- A100: a2-highgpu-1g: $3.748064

**GPU Notes:**

- A100/Cuda doesn't support FP8
- A100 has less availability than L4: need to wait for batch job (can be long) or switch to us-east location

<Callout type="warn">
  Don't forget to enable **Batch & Storage API** if used without gcloud
  ([link](https://cloud.google.com/batch/docs/get-started#project-prerequisites))
</Callout>
